{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Mining \u00b6 Pengertian Data Mining \u00b6 Penggalian data (bahasa Inggris: data mining ) adalah ekstraksi pola yang menarik dari data dalam jumlah besar. Penggalian data memiliki beberapa nama alternatif, meskipun definisi eksaknya berbeda, seperti KDD ( knowledge discovery in database ), analisis pola, arkeologi data, pemanen-an informasi, dan bisnis cerdas. Penggalian data diperlukan saat data yang tersedia terlalu banyak (misalnya data yang diperoleh dari sistem basis data perusahaan, e-commerce, data saham, data sensus dan data bioinformatika, tetapi tidak tahu pola apa yang bisa didapatkan. Secara umum definisi data-mining dapat diartikan sebagai berikut: Proses penemuan pola yang menarik dari data yang tersimpan dalam jumlah besar. Ekstraksi dari suatu informasi yang berguna atau menarik (non-trivial, implisit, sebefumnya belum diketahui potensial kegunaannya) pola atau pengetahuan dari data yang disimpan dalam jumfah besar. Ekplorasi dari analisa secara otomatis atau semiotomatis terhadap data-data dalam jumlah besar untuk mencari pola dan aturan yang berarti. Kemampuan Data mining untuk mencari informasi bisnis yang berharga dari basis data yang sangat besar, dapat dianalogikan dengan penambangan logam mulia dari lahan sumbernya, teknologi ini dipakai untuk: Prediksi trend dan sifat-sifat bisnis, dimana data mining mengotomatisasi proses pencarian informasi pemprediksi di dalam basis data yang besar. Penemuan pola-pola yang tidak diketahui sebelumnya, dimana data mining menyapu basis data, kemudian mengidentifikasi pola-pola yang sebelumnya tersembunyi dalam satu sapuan. Data mining berguna untuk membuat keputusan yang kritis, terutama dalam strategi. Konsep Data Mining \u00b6 Data mining sangat perlu dilakukan terutama dalam mengelola data yang sangat besar untuk memudahkan aktifitas recording suatu transaksi dan untuk proses data warehousing agar dapat memberikan informasi yang akurat bagi penggunanya. Alasan utama mengapa data mining sangat menarik perhatian industri informasi dalam beberapa tahun belakangan ini adalah karena tersedianya data dalam jumlah yang besar dan semakin besarnya kebutuhan untuk mengubah data tersebut menjadi informasi dan pengetahuan yang berguna karena sesuai fokus bidang ilmu ini yaitu melakukan kegiatan mengekstraksi atau menambang pengetahuan dari data yang berukuran/berjumlah besar, informasi inilah yang nantinya sangat berguna untuk pengembangan. Fungsi Data Mining \u00b6 Data mining mempunyai fungsi yang penting untuk membantu mendapatkan informasi yang berguna serta meningkatkan pengetahuan bagi pengguna. Pada dasarnya, data mining mempunyai empat fungsi dasar yaitu: Fungsi Prediksi (prediction) , Proses untuk menemukan pola dari data dengan menggunakan beberapa variabel untuk memprediksikan variabel lain yang tidak diketahui jenis atau nilainya. Fungsi Deskripsi (description) , Proses untuk menemukan suatu karakteristik penting dari data dalam suatu basis data. Fungsi Klasifikasi (classification) , Klasifikasi merupakan suatu proses untuk menemukan model atau fungsi untuk menggambarkan class atau konsep dari suatu data. Proses yang digunakan untuk mendeskripsikan data yang penting serta dapat meramalkan kecenderungan data pada masa depan. Fungsi Asosiasi (association) , Proses ini digunakan untuk menemukan suatu hubungan yang terdapat pada nilai atribut dari sekumpulan data. Tahapan Data Mining \u00b6 Data selection Pemilihan (seleksi) data dari sekumpulan data operasional perlu dilakukan sebelum tahap penggalian informasi dalam KDD dimulai. Data hasil seleksi yang digunakan untuk proses data mining, disimpan dalam suatu berkas, terpisah dari basis data operasional. Pre-processing / Cleaning Sebelum proses data mining dapat dilaksanakan, perlu dilakukan proses cleaning pada data yang menjadi fokus KDD. Proses cleaning mencakup antara lain membuang duplikasi data, memeriksa data yang inkonsisten, dan memperbaiki kesalahan pada data. Transformation Coding adalah proses transformasi pada data yang telah dipilih, sehingga data tersebut sesuai untuk proses data mining. Proses coding dalam KDD merupakan proses kreatif dan sangat tergantung pada jenis atau pola informasi yang akan dicari dalam basis data. Data Mining Data mining adalah proses mencari pola atau informasi menarik dalam data terpilih dengan menggunakan teknik atau metode tertentu. Teknik, metode, atau algoritma dalam data mining sangat bervariasi. Pemilihan metode atau algoritma yang tepat sangat bergantung pada tujuan dan proses KDD secara keseluruhan. Interpretation / Evalution Pola informasi yang dihasilkan dari proses data mining perlu ditampilkan dalam bentuk yang mudah dimengerti oleh pihak yang berkepentingan. Tahap ini merupakan bagian dari proses KDD yang disebut interpretation . Tahap ini mencakup pemeriksaan apakah pola atau informasi yang ditemukan bertentangan dengan fakta atau hipotesis yang ada sebelumnya. Referensi \u00b6 [ 1 ] https://www.mkdocs.org [ 2 ] https://id.wikipedia.org/wiki/Penggalian_data [ 3 ] http://gsbipb.com/?p=821 [ 4 ] https://beyonder.asia/pengertian-fungsi-proses-dan-tahapan-data-mining/","title":"Data Mining"},{"location":"#data-mining","text":"","title":"Data Mining"},{"location":"#pengertian-data-mining","text":"Penggalian data (bahasa Inggris: data mining ) adalah ekstraksi pola yang menarik dari data dalam jumlah besar. Penggalian data memiliki beberapa nama alternatif, meskipun definisi eksaknya berbeda, seperti KDD ( knowledge discovery in database ), analisis pola, arkeologi data, pemanen-an informasi, dan bisnis cerdas. Penggalian data diperlukan saat data yang tersedia terlalu banyak (misalnya data yang diperoleh dari sistem basis data perusahaan, e-commerce, data saham, data sensus dan data bioinformatika, tetapi tidak tahu pola apa yang bisa didapatkan. Secara umum definisi data-mining dapat diartikan sebagai berikut: Proses penemuan pola yang menarik dari data yang tersimpan dalam jumlah besar. Ekstraksi dari suatu informasi yang berguna atau menarik (non-trivial, implisit, sebefumnya belum diketahui potensial kegunaannya) pola atau pengetahuan dari data yang disimpan dalam jumfah besar. Ekplorasi dari analisa secara otomatis atau semiotomatis terhadap data-data dalam jumlah besar untuk mencari pola dan aturan yang berarti. Kemampuan Data mining untuk mencari informasi bisnis yang berharga dari basis data yang sangat besar, dapat dianalogikan dengan penambangan logam mulia dari lahan sumbernya, teknologi ini dipakai untuk: Prediksi trend dan sifat-sifat bisnis, dimana data mining mengotomatisasi proses pencarian informasi pemprediksi di dalam basis data yang besar. Penemuan pola-pola yang tidak diketahui sebelumnya, dimana data mining menyapu basis data, kemudian mengidentifikasi pola-pola yang sebelumnya tersembunyi dalam satu sapuan. Data mining berguna untuk membuat keputusan yang kritis, terutama dalam strategi.","title":"Pengertian Data Mining"},{"location":"#konsep-data-mining","text":"Data mining sangat perlu dilakukan terutama dalam mengelola data yang sangat besar untuk memudahkan aktifitas recording suatu transaksi dan untuk proses data warehousing agar dapat memberikan informasi yang akurat bagi penggunanya. Alasan utama mengapa data mining sangat menarik perhatian industri informasi dalam beberapa tahun belakangan ini adalah karena tersedianya data dalam jumlah yang besar dan semakin besarnya kebutuhan untuk mengubah data tersebut menjadi informasi dan pengetahuan yang berguna karena sesuai fokus bidang ilmu ini yaitu melakukan kegiatan mengekstraksi atau menambang pengetahuan dari data yang berukuran/berjumlah besar, informasi inilah yang nantinya sangat berguna untuk pengembangan.","title":"Konsep Data Mining"},{"location":"#fungsi-data-mining","text":"Data mining mempunyai fungsi yang penting untuk membantu mendapatkan informasi yang berguna serta meningkatkan pengetahuan bagi pengguna. Pada dasarnya, data mining mempunyai empat fungsi dasar yaitu: Fungsi Prediksi (prediction) , Proses untuk menemukan pola dari data dengan menggunakan beberapa variabel untuk memprediksikan variabel lain yang tidak diketahui jenis atau nilainya. Fungsi Deskripsi (description) , Proses untuk menemukan suatu karakteristik penting dari data dalam suatu basis data. Fungsi Klasifikasi (classification) , Klasifikasi merupakan suatu proses untuk menemukan model atau fungsi untuk menggambarkan class atau konsep dari suatu data. Proses yang digunakan untuk mendeskripsikan data yang penting serta dapat meramalkan kecenderungan data pada masa depan. Fungsi Asosiasi (association) , Proses ini digunakan untuk menemukan suatu hubungan yang terdapat pada nilai atribut dari sekumpulan data.","title":"Fungsi Data Mining"},{"location":"#tahapan-data-mining","text":"Data selection Pemilihan (seleksi) data dari sekumpulan data operasional perlu dilakukan sebelum tahap penggalian informasi dalam KDD dimulai. Data hasil seleksi yang digunakan untuk proses data mining, disimpan dalam suatu berkas, terpisah dari basis data operasional. Pre-processing / Cleaning Sebelum proses data mining dapat dilaksanakan, perlu dilakukan proses cleaning pada data yang menjadi fokus KDD. Proses cleaning mencakup antara lain membuang duplikasi data, memeriksa data yang inkonsisten, dan memperbaiki kesalahan pada data. Transformation Coding adalah proses transformasi pada data yang telah dipilih, sehingga data tersebut sesuai untuk proses data mining. Proses coding dalam KDD merupakan proses kreatif dan sangat tergantung pada jenis atau pola informasi yang akan dicari dalam basis data. Data Mining Data mining adalah proses mencari pola atau informasi menarik dalam data terpilih dengan menggunakan teknik atau metode tertentu. Teknik, metode, atau algoritma dalam data mining sangat bervariasi. Pemilihan metode atau algoritma yang tepat sangat bergantung pada tujuan dan proses KDD secara keseluruhan. Interpretation / Evalution Pola informasi yang dihasilkan dari proses data mining perlu ditampilkan dalam bentuk yang mudah dimengerti oleh pihak yang berkepentingan. Tahap ini merupakan bagian dari proses KDD yang disebut interpretation . Tahap ini mencakup pemeriksaan apakah pola atau informasi yang ditemukan bertentangan dengan fakta atau hipotesis yang ada sebelumnya.","title":"Tahapan Data Mining"},{"location":"#referensi","text":"[ 1 ] https://www.mkdocs.org [ 2 ] https://id.wikipedia.org/wiki/Penggalian_data [ 3 ] http://gsbipb.com/?p=821 [ 4 ] https://beyonder.asia/pengertian-fungsi-proses-dan-tahapan-data-mining/","title":"Referensi"},{"location":"decision-tree/","text":"Decision Tree \u00b6 Definisi Decision Tree \u00b6 Pohon keputusan (Decision Tree) adalah alat pendukung keputusan yang menggunakan model keputusan seperti pohon dan kemungkinan konsekuensinya, termasuk hasil kebetulan, sumber daya, dan utilitas. Ini adalah salah satu cara untuk menampilkan algoritma yang hanya berisi pernyataan kontrol bersyarat. Dalam mengklasifikasikan data, model prediksi menggunakan struktur pohon/ berhirarki ini memiliki kemampuan untuk mem-break-down proses pengambilan keputusan yang kompleks menjadi simpel. Pengambilan keputusan akan lebih menginterpretasikan solusi dari permasalahan. Semakin banyak cabang pada pohon keputusan maka akan semakin banyak rule (aturan). Pembahasan pada halaman ini akan melihat bagaimana Decision Tree dapat diimplementasikan dengan library \"Scikit-Learn Python\" . Dasar Teori \u00b6 Decision Tree adalah salah satu algoritma paling kuat dan populer. Algoritma decision-tree termasuk dalam kategori algoritma pembelajaran terawasi. Ia bekerja untuk variabel output kontinuitas maupun kategoris. Asumsi yang saat menggunakan pohon keputusan: Pada awalnya, seluruh rangkaian pelatihan dianggap sebagai akar. Nilai fitur lebih disukai sebagai kategori. Jika nilai kontinu maka mereka didiskritisasi sebelum membangun model. Catatan didistribusikan secara rekursif berdasarkan nilai atribut. Urutan untuk menempatkan atribut sebagai root atau simpul internal pohon dilakukan dengan menggunakan beberapa pendekatan statistik. Algoritma Pohon Keputusan: Tempatkan atribut terbaik dari dataset kami di akar pohon. Membagi set pelatihan menjadi himpunan bagian. Subset harus dibuat sedemikian rupa sehingga setiap subset berisi data dengan nilai yang sama untuk suatu atribut. Ulangi langkah 1 dan langkah 2 pada setiap subset sampai Anda menemukan simpul daun di semua cabang pohon. Saat membangun classifier pohon keputusan, dapat meningkatkan akurasinya dengan menyetelnya dengan parameter yang berbeda. Tetapi penyempurnaan ini harus dilakukan dengan hati-hati karena dengan melakukan ini, algoritma dapat menyesuaikan dengan data training & pada akhirnya itu akan membangun model generalisasi yang buruk. Dalam memprediksi data dengan metode Decision Tree menggunakan salah satu dari 2 kriteria yaitu \u201cgini index\u201d atau \u201cinformation gain\u201d. Kriteria ini digunakan untuk memilih banyaknya attributes dari dataset yang atributnya akan ditempatkan pada root node atau internal node. Gini Index Rumus Gini Index : Indeks Gini adalah metrik untuk mengukur seberapa sering elemen yang dipilih secara acak akan diidentifikasi secara salah. Itu berarti atribut dengan indeks gini yang lebih rendah harus lebih disukai. Information Gain Rumum Information Gian : Information Gain adalah ukuran ketidakpastian variabel acak, itu mencirikan ketidakmurnian dari kumpulan contoh yang sewenang-wenang. Semakin tinggi entropi, semakin banyak konten informasinya. Implementasi Program \u00b6 Pada bagian ini, membahas bagaimana Decision Tree dapat diimplementasikan dengan library \"Scikit-Learn Python . Dataset dapat dilihat di sini . Berikut kodenya: Download # Jalankan Program ini di python lokal. # Install libraries. # Import libraries yang dibutuhkan dalam program import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report # Fungsi import dataset yang digunakan def importdata(): balance_data = pd.read_csv( 'https://archive.ics.uci.edu/ml/machine-learning-'+ 'databases/balance-scale/balance-scale.data', sep= ',', header = None) # Tampilkan info Dataset print (\"Informasi Data\\n\") print (\"Jumlah data: \", len(balance_data)) print (\"Dimensi data: \", balance_data.shape) # Tampilkan Dataset pengamatan print (\"Dataset: \\n\",balance_data.head()) return balance_data # Fungsi untuk split dataset def splitdataset(balance_data): # Pembagian variabel target X = balance_data.values[:, 1:5] Y = balance_data.values[:, 0] # Split dataset menjadi data training dan data testing X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100) return X, Y, X_train, X_test, y_train, y_test # Fungsi untuk menunjukkan data training dengan GiniIndex def train_using_gini(X_train, X_test, y_train): # Membuat objek klasifikasi clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=3, min_samples_leaf=5) # menunjukkan data training clf_gini.fit(X_train, y_train) return clf_gini # Fungsi untuk menunjukkan data training dengan Entropy. def tarin_using_entropy(X_train, X_test, y_train): # Pohon keputusan dengan entropy clf_entropy = DecisionTreeClassifier( criterion = \"entropy\", random_state = 100, max_depth = 3, min_samples_leaf = 5) # menunjukkan data training clf_entropy.fit(X_train, y_train) return clf_entropy # Fungsi untuk membuat prediksi def prediction(X_test, clf_object): y_pred = clf_object.predict(X_test) print(\"Hasil prediksi:\") print(y_pred) return y_pred # Fungsi menghitung akurasi def cal_accuracy(y_test, y_pred): print (\"Matriks: \\n\", confusion_matrix(y_test, y_pred)) print (\"Akurasi: \", accuracy_score(y_test,y_pred)*100) print (\"Laporan: \", classification_report(y_test, y_pred)) # Kode Utama def main(): data = importdata() X, Y, X_train, X_test, y_train, y_test = splitdataset(data) clf_gini = train_using_gini(X_train, X_test, y_train) clf_entropy = tarin_using_entropy(X_train, X_test, y_train) print(\"\\nHasil hitung dengan Gini Index\") y_pred_gini = prediction(X_test, clf_gini) cal_accuracy(y_test, y_pred_gini) print(\"\\nHasil hitung dengan Entropy\") y_pred_entropy = prediction(X_test, clf_entropy) cal_accuracy(y_test, y_pred_entropy) main() Hasilnya: Informasi Data Jumlah data: 625 Dimensi data: (625, 5) Dataset: 0 1 2 3 4 0 B 1 1 1 1 1 R 1 1 1 2 2 R 1 1 1 3 3 R 1 1 1 4 4 R 1 1 1 5 Hasil hitung dengan Gini Index Hasil prediksi: ['R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'R'] Matriks: [[ 0 6 7] [ 0 67 18] [ 0 19 71]] Akurasi: 73.40425531914893 Laporan: precision recall f1-score support B 0.00 0.00 0.00 13 L 0.73 0.79 0.76 85 R 0.74 0.79 0.76 90 micro avg 0.73 0.73 0.73 188 macro avg 0.49 0.53 0.51 188 weighted avg 0.68 0.73 0.71 188 Hasil hitung dengan Entropy Hasil prediksi: ['R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'R'] Matriks: [[ 0 6 7] [ 0 63 22] [ 0 20 70]] Akurasi: 70.74468085106383 Laporan: precision recall f1-score support B 0.00 0.00 0.00 13 L 0.71 0.74 0.72 85 R 0.71 0.78 0.74 90 micro avg 0.71 0.71 0.71 188 macro avg 0.47 0.51 0.49 188 weighted avg 0.66 0.71 0.68 188 Pro dan Kontra Decision Tree \u00b6 Pro Pohon keputusan mudah diinterpretasikan dan divisualisasikan. Itu dapat dengan mudah menangkap pola Non-linear. Ini membutuhkan lebih sedikit pemrosesan data dari pengguna, misalnya, tidak perlu menormalkan kolom. Ini dapat digunakan untuk rekayasa fitur seperti memprediksi nilai yang hilang, cocok untuk pemilihan variabel. Pohon keputusan tidak memiliki asumsi tentang distribusi karena sifat non-parametrik dari algoritma. Kontra Peka terhadap data berisik. Itu bisa menyesuaikan data yang bising. Variasi kecil (atau varians) dalam data dapat menghasilkan pohon keputusan yang berbeda. Ini dapat dikurangi dengan mengantongi dan meningkatkan algoritma. Pohon keputusan bias dengan dataset ketidakseimbangan, jadi disarankan untuk menyeimbangkan dataset sebelum membuat pohon keputusan. Kesimpulan \u00b6 Decision Tree memiliki kemampuan untuk mem-break-down proses pengambilan keputusan yang kompleks menjadi simpel. Semakin banyak cabang pada pohon keputusan maka akan semakin banyak rule (aturan). Referensi \u00b6 [ 1 ] https://www.mkdocs.org [ 2 ] https://en.wikipedia.org/wiki/Decision_tree [ 3 ] https://www.geeksforgeeks.org/decision-tree-implementation-python/ [ 4 ] https://www.datacamp.com/community/tutorials/decision-tree-classification-python [ 5 ] https://scikit-learn.org/stable/modules/tree.html","title":"Decision Tree"},{"location":"decision-tree/#decision-tree","text":"","title":"Decision Tree"},{"location":"decision-tree/#definisi-decision-tree","text":"Pohon keputusan (Decision Tree) adalah alat pendukung keputusan yang menggunakan model keputusan seperti pohon dan kemungkinan konsekuensinya, termasuk hasil kebetulan, sumber daya, dan utilitas. Ini adalah salah satu cara untuk menampilkan algoritma yang hanya berisi pernyataan kontrol bersyarat. Dalam mengklasifikasikan data, model prediksi menggunakan struktur pohon/ berhirarki ini memiliki kemampuan untuk mem-break-down proses pengambilan keputusan yang kompleks menjadi simpel. Pengambilan keputusan akan lebih menginterpretasikan solusi dari permasalahan. Semakin banyak cabang pada pohon keputusan maka akan semakin banyak rule (aturan). Pembahasan pada halaman ini akan melihat bagaimana Decision Tree dapat diimplementasikan dengan library \"Scikit-Learn Python\" .","title":"Definisi Decision Tree"},{"location":"decision-tree/#dasar-teori","text":"Decision Tree adalah salah satu algoritma paling kuat dan populer. Algoritma decision-tree termasuk dalam kategori algoritma pembelajaran terawasi. Ia bekerja untuk variabel output kontinuitas maupun kategoris. Asumsi yang saat menggunakan pohon keputusan: Pada awalnya, seluruh rangkaian pelatihan dianggap sebagai akar. Nilai fitur lebih disukai sebagai kategori. Jika nilai kontinu maka mereka didiskritisasi sebelum membangun model. Catatan didistribusikan secara rekursif berdasarkan nilai atribut. Urutan untuk menempatkan atribut sebagai root atau simpul internal pohon dilakukan dengan menggunakan beberapa pendekatan statistik. Algoritma Pohon Keputusan: Tempatkan atribut terbaik dari dataset kami di akar pohon. Membagi set pelatihan menjadi himpunan bagian. Subset harus dibuat sedemikian rupa sehingga setiap subset berisi data dengan nilai yang sama untuk suatu atribut. Ulangi langkah 1 dan langkah 2 pada setiap subset sampai Anda menemukan simpul daun di semua cabang pohon. Saat membangun classifier pohon keputusan, dapat meningkatkan akurasinya dengan menyetelnya dengan parameter yang berbeda. Tetapi penyempurnaan ini harus dilakukan dengan hati-hati karena dengan melakukan ini, algoritma dapat menyesuaikan dengan data training & pada akhirnya itu akan membangun model generalisasi yang buruk. Dalam memprediksi data dengan metode Decision Tree menggunakan salah satu dari 2 kriteria yaitu \u201cgini index\u201d atau \u201cinformation gain\u201d. Kriteria ini digunakan untuk memilih banyaknya attributes dari dataset yang atributnya akan ditempatkan pada root node atau internal node. Gini Index Rumus Gini Index : Indeks Gini adalah metrik untuk mengukur seberapa sering elemen yang dipilih secara acak akan diidentifikasi secara salah. Itu berarti atribut dengan indeks gini yang lebih rendah harus lebih disukai. Information Gain Rumum Information Gian : Information Gain adalah ukuran ketidakpastian variabel acak, itu mencirikan ketidakmurnian dari kumpulan contoh yang sewenang-wenang. Semakin tinggi entropi, semakin banyak konten informasinya.","title":"Dasar Teori"},{"location":"decision-tree/#implementasi-program","text":"Pada bagian ini, membahas bagaimana Decision Tree dapat diimplementasikan dengan library \"Scikit-Learn Python . Dataset dapat dilihat di sini . Berikut kodenya: Download # Jalankan Program ini di python lokal. # Install libraries. # Import libraries yang dibutuhkan dalam program import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report # Fungsi import dataset yang digunakan def importdata(): balance_data = pd.read_csv( 'https://archive.ics.uci.edu/ml/machine-learning-'+ 'databases/balance-scale/balance-scale.data', sep= ',', header = None) # Tampilkan info Dataset print (\"Informasi Data\\n\") print (\"Jumlah data: \", len(balance_data)) print (\"Dimensi data: \", balance_data.shape) # Tampilkan Dataset pengamatan print (\"Dataset: \\n\",balance_data.head()) return balance_data # Fungsi untuk split dataset def splitdataset(balance_data): # Pembagian variabel target X = balance_data.values[:, 1:5] Y = balance_data.values[:, 0] # Split dataset menjadi data training dan data testing X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100) return X, Y, X_train, X_test, y_train, y_test # Fungsi untuk menunjukkan data training dengan GiniIndex def train_using_gini(X_train, X_test, y_train): # Membuat objek klasifikasi clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=3, min_samples_leaf=5) # menunjukkan data training clf_gini.fit(X_train, y_train) return clf_gini # Fungsi untuk menunjukkan data training dengan Entropy. def tarin_using_entropy(X_train, X_test, y_train): # Pohon keputusan dengan entropy clf_entropy = DecisionTreeClassifier( criterion = \"entropy\", random_state = 100, max_depth = 3, min_samples_leaf = 5) # menunjukkan data training clf_entropy.fit(X_train, y_train) return clf_entropy # Fungsi untuk membuat prediksi def prediction(X_test, clf_object): y_pred = clf_object.predict(X_test) print(\"Hasil prediksi:\") print(y_pred) return y_pred # Fungsi menghitung akurasi def cal_accuracy(y_test, y_pred): print (\"Matriks: \\n\", confusion_matrix(y_test, y_pred)) print (\"Akurasi: \", accuracy_score(y_test,y_pred)*100) print (\"Laporan: \", classification_report(y_test, y_pred)) # Kode Utama def main(): data = importdata() X, Y, X_train, X_test, y_train, y_test = splitdataset(data) clf_gini = train_using_gini(X_train, X_test, y_train) clf_entropy = tarin_using_entropy(X_train, X_test, y_train) print(\"\\nHasil hitung dengan Gini Index\") y_pred_gini = prediction(X_test, clf_gini) cal_accuracy(y_test, y_pred_gini) print(\"\\nHasil hitung dengan Entropy\") y_pred_entropy = prediction(X_test, clf_entropy) cal_accuracy(y_test, y_pred_entropy) main() Hasilnya: Informasi Data Jumlah data: 625 Dimensi data: (625, 5) Dataset: 0 1 2 3 4 0 B 1 1 1 1 1 R 1 1 1 2 2 R 1 1 1 3 3 R 1 1 1 4 4 R 1 1 1 5 Hasil hitung dengan Gini Index Hasil prediksi: ['R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'R'] Matriks: [[ 0 6 7] [ 0 67 18] [ 0 19 71]] Akurasi: 73.40425531914893 Laporan: precision recall f1-score support B 0.00 0.00 0.00 13 L 0.73 0.79 0.76 85 R 0.74 0.79 0.76 90 micro avg 0.73 0.73 0.73 188 macro avg 0.49 0.53 0.51 188 weighted avg 0.68 0.73 0.71 188 Hasil hitung dengan Entropy Hasil prediksi: ['R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'R'] Matriks: [[ 0 6 7] [ 0 63 22] [ 0 20 70]] Akurasi: 70.74468085106383 Laporan: precision recall f1-score support B 0.00 0.00 0.00 13 L 0.71 0.74 0.72 85 R 0.71 0.78 0.74 90 micro avg 0.71 0.71 0.71 188 macro avg 0.47 0.51 0.49 188 weighted avg 0.66 0.71 0.68 188","title":"Implementasi Program"},{"location":"decision-tree/#pro-dan-kontra-decision-tree","text":"Pro Pohon keputusan mudah diinterpretasikan dan divisualisasikan. Itu dapat dengan mudah menangkap pola Non-linear. Ini membutuhkan lebih sedikit pemrosesan data dari pengguna, misalnya, tidak perlu menormalkan kolom. Ini dapat digunakan untuk rekayasa fitur seperti memprediksi nilai yang hilang, cocok untuk pemilihan variabel. Pohon keputusan tidak memiliki asumsi tentang distribusi karena sifat non-parametrik dari algoritma. Kontra Peka terhadap data berisik. Itu bisa menyesuaikan data yang bising. Variasi kecil (atau varians) dalam data dapat menghasilkan pohon keputusan yang berbeda. Ini dapat dikurangi dengan mengantongi dan meningkatkan algoritma. Pohon keputusan bias dengan dataset ketidakseimbangan, jadi disarankan untuk menyeimbangkan dataset sebelum membuat pohon keputusan.","title":"Pro dan Kontra Decision Tree"},{"location":"decision-tree/#kesimpulan","text":"Decision Tree memiliki kemampuan untuk mem-break-down proses pengambilan keputusan yang kompleks menjadi simpel. Semakin banyak cabang pada pohon keputusan maka akan semakin banyak rule (aturan).","title":"Kesimpulan"},{"location":"decision-tree/#referensi","text":"[ 1 ] https://www.mkdocs.org [ 2 ] https://en.wikipedia.org/wiki/Decision_tree [ 3 ] https://www.geeksforgeeks.org/decision-tree-implementation-python/ [ 4 ] https://www.datacamp.com/community/tutorials/decision-tree-classification-python [ 5 ] https://scikit-learn.org/stable/modules/tree.html","title":"Referensi"},{"location":"knearest-neighbors/","text":"K-Nearest Neighbors \u00b6 Definisi K-Nearest Neighbors \u00b6 Algoritma K-Nearest Neighbors (KNN) adalah salah satu metode yang digunakan dalam implementasi machine learning . KNN dapat lebih mudah dipraktikkan pada bentuk algoritma dasar, namun implementasinya perlu menyelesaikan proses klasifikasi yang dirasa agak rumit bagi sebagian orang. Pada dasarnya, penggunaan algoritma ini tidak memerlukan pelatihan khusus. Orang awam juga dapat mempraktikkan dengan panduan program manual. Dalam mengklasifikasikan data baru, KNN ini perlu fakta/ data yang sudah ada yang digunakan sebagai data training sebagai acuan klasifikasi. KNN menjadi algoritma pembelajaran non-parametrik, maksudnya yaitu metode ini tidak menjadi sebuah asumsi tentang apa pun dengan data yang menjadi acuan klasifikasi. KNN menjadi sebuah fitur yang sangat berguna karena sebagian besar data dunia nyata tidak benar-benar mengikuti asumsi teoretis, misalnya: linear-separability , uniform distribution , dll. Pembahasan pada halaman ini akan melihat bagaimana KNN dapat diimplementasikan dengan library \"Scikit-Learn Python\" . Namun, kita juga perlu tahu bagaimana teori dari metode KNN ini dan bagaimana respon mengenai pro dan kontra dari algoritma tersebut. Dasar Teori \u00b6 Algoritma KNN adalah salah satu yang paling sederhana dari semua algoritma machine learning . Ini hanya menghitung jarak dari titik data baru ke semua titik data pelatihan lainnya. Jarak dapat dari jenis apa pun, misalnya: Euclidean. Rumus Euclidean Distance: Kemudian akan memilih titik data K-terdekat, di mana K dapat berupa bilangan bulat apa pun. Kemudian akan diberikan titik data ke kelas tempat mayoritas titik data K berada. Mari kita lihat algoritma ini dalam contoh sederhana, misalkan Anda memiliki dataset dengan dua variabel, yang ketika diplot, terlihat seperti pada gambar berikut: Tugas Anda adalah untuk mengklasifikasikan titik data baru dengan 'X' ke dalam kelas \"Biru\" atau \"Merah\". Nilai koordinat titik data adalah x = 45 dan y = 50. Misalkan nilai K adalah 3. Algoritma KNN dimulai dengan menghitung jarak titik X dari semua titik. Ia kemudian menemukan 3 titik terdekat dengan jarak paling sedikit ke titik X. Ini ditunjukkan pada gambar di bawah ini. Tiga titik terdekat telah dilingkari. Langkah terakhir dari algoritma KNN adalah untuk menetapkan titik baru ke kelas yang dimiliki mayoritas tiga titik terdekat. Dari gambar di atas kita dapat melihat bahwa dua dari tiga titik terdekat milik kelas \"Merah\" sementara satu milik kelas \"Biru\". Oleh karena itu titik data baru akan diklasifikasikan sebagai \"Merah\". Pro dan Kontra KNN \u00b6 Pro Sangat mudah diimplementasikan. KNN tidak memerlukan training sebelum membuat prediksi realtime . Ini membuat algoritma KNN jauh lebih cepat daripada algoritma lain yang membutuhkan training mis. SVM, regresi linier, dll. Karena algoritma tidak memerlukan training sebelum membuat prediksi, data baru dapat ditambahkan dengan mulus. Hanya ada dua parameter yang diperlukan untuk mengimplementasikan KNN yaitu nilai K dan fungsi jarak (mis. Euclidean atau Manhattan dll.) Kontra Algoritma KNN tidak bekerja dengan baik dengan data dimensi tinggi karena dengan sejumlah besar dimensi, menjadi sulit bagi algoritma untuk menghitung jarak di setiap dimensi. Algoritma KNN memiliki cost prediksi tinggi untuk kumpulan data besar. Ini karena dalam dataset besar cost jarak penghitungan antara titik baru dan setiap titik yang ada menjadi lebih tinggi. Terakhir, algoritma KNN tidak bekerja dengan baik dengan fitur kategorikal karena sulit untuk menemukan jarak antara dimensi dengan fitur kategorikal. Implementasi Program \u00b6 Pada bagian ini, membahas bagaimana implementasi algoritma k-nearest neighbors dengan Scikit-Learn Python. Kita akan menggunakan dataset iris yang terkenal untuk contoh KNN ini. Dataset terdiri dari empat atribut: sepal-width, sepal-length, petal-width dan petal-length. Ini adalah atribut dari jenis spesifik tanaman iris. Fungsinya adalah untuk memprediksi kelas tanaman ini. Ada tiga kelas dalam dataset: Iris-setosa, Iris-versicolor dan Iris-virginica. Rincian lebih lanjut dari dataset tersedia di sini . Berikut langkah-langkah implementasi KNN: Download [1] Mengimpor Library python import pandas as pd import matplotlib.pyplot as plt import numpy as np [2] Mengimpor Dataset Untuk mengimpor dataset dan memuatnya dalam program, jalankan kode berikut: from sklearn import datasets from sklearn.linear_model import logistic_regression_path data_iris=datasets.load_iris() print(data_iris.data) print(data_iris.target) Hasilnya: [3] Split Dataset Untuk membagi dataset menjadi data training dan data testing dalam program, jalankan kode berikut: from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test=train_test_split(data_iris.data,data_iris.target,test_size=0.2) [4] Hitung Akurasi Untuk menghitung akurasi dalam program, jalankan kode berikut: from sklearn.neighbors import KNeighborsClassifier clf=KNeighborsClassifier(n_neighbors=3).fit(x_train,y_train) from sklearn.metrics import accuracy_score print(\"accuracy is = \", accuracy_score(y_test,clf.predict(x_test))) [5] Menampilkan grafik program Untuk grafik perhitungan dalam program, jalankan kode berikut: accuracy_values=[] for x in range(1,x_train.shape[0]): clf=KNeighborsClassifier(n_neighbors=x).fit(x_train,y_train) accuracy=accuracy_score(y_test,clf.predict(x_test)) accuracy_values.append([x,accuracy]) pass accuracy_values=np.array(accuracy_values) plt.plot(accuracy_values[:,0],accuracy_values[:,1]) plt.xlabel(\"K\") plt.ylabel(\"accuracy\") plt.show() Hasilnya : catatan: Grafik yang ditampilkan setiap menjalankan program akan dapat berbeda karena data diambil secara random. Kesimpulan \u00b6 KNN adalah algoritma klasifikasi yang sederhana dan mudah diimplementasikan. Algoritma KNN telah banyak digunakan untuk menemukan kesamaan dokumen dan pengenalan pola. Untuk tutorial lainnya klik di sini . Referensi \u00b6 [ 1 ] https://www.mkdocs.org [ 2 ] https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/","title":"K-Nearest Neighbors"},{"location":"knearest-neighbors/#k-nearest-neighbors","text":"","title":"K-Nearest Neighbors"},{"location":"knearest-neighbors/#definisi-k-nearest-neighbors","text":"Algoritma K-Nearest Neighbors (KNN) adalah salah satu metode yang digunakan dalam implementasi machine learning . KNN dapat lebih mudah dipraktikkan pada bentuk algoritma dasar, namun implementasinya perlu menyelesaikan proses klasifikasi yang dirasa agak rumit bagi sebagian orang. Pada dasarnya, penggunaan algoritma ini tidak memerlukan pelatihan khusus. Orang awam juga dapat mempraktikkan dengan panduan program manual. Dalam mengklasifikasikan data baru, KNN ini perlu fakta/ data yang sudah ada yang digunakan sebagai data training sebagai acuan klasifikasi. KNN menjadi algoritma pembelajaran non-parametrik, maksudnya yaitu metode ini tidak menjadi sebuah asumsi tentang apa pun dengan data yang menjadi acuan klasifikasi. KNN menjadi sebuah fitur yang sangat berguna karena sebagian besar data dunia nyata tidak benar-benar mengikuti asumsi teoretis, misalnya: linear-separability , uniform distribution , dll. Pembahasan pada halaman ini akan melihat bagaimana KNN dapat diimplementasikan dengan library \"Scikit-Learn Python\" . Namun, kita juga perlu tahu bagaimana teori dari metode KNN ini dan bagaimana respon mengenai pro dan kontra dari algoritma tersebut.","title":"Definisi K-Nearest Neighbors"},{"location":"knearest-neighbors/#dasar-teori","text":"Algoritma KNN adalah salah satu yang paling sederhana dari semua algoritma machine learning . Ini hanya menghitung jarak dari titik data baru ke semua titik data pelatihan lainnya. Jarak dapat dari jenis apa pun, misalnya: Euclidean. Rumus Euclidean Distance: Kemudian akan memilih titik data K-terdekat, di mana K dapat berupa bilangan bulat apa pun. Kemudian akan diberikan titik data ke kelas tempat mayoritas titik data K berada. Mari kita lihat algoritma ini dalam contoh sederhana, misalkan Anda memiliki dataset dengan dua variabel, yang ketika diplot, terlihat seperti pada gambar berikut: Tugas Anda adalah untuk mengklasifikasikan titik data baru dengan 'X' ke dalam kelas \"Biru\" atau \"Merah\". Nilai koordinat titik data adalah x = 45 dan y = 50. Misalkan nilai K adalah 3. Algoritma KNN dimulai dengan menghitung jarak titik X dari semua titik. Ia kemudian menemukan 3 titik terdekat dengan jarak paling sedikit ke titik X. Ini ditunjukkan pada gambar di bawah ini. Tiga titik terdekat telah dilingkari. Langkah terakhir dari algoritma KNN adalah untuk menetapkan titik baru ke kelas yang dimiliki mayoritas tiga titik terdekat. Dari gambar di atas kita dapat melihat bahwa dua dari tiga titik terdekat milik kelas \"Merah\" sementara satu milik kelas \"Biru\". Oleh karena itu titik data baru akan diklasifikasikan sebagai \"Merah\".","title":"Dasar Teori"},{"location":"knearest-neighbors/#pro-dan-kontra-knn","text":"Pro Sangat mudah diimplementasikan. KNN tidak memerlukan training sebelum membuat prediksi realtime . Ini membuat algoritma KNN jauh lebih cepat daripada algoritma lain yang membutuhkan training mis. SVM, regresi linier, dll. Karena algoritma tidak memerlukan training sebelum membuat prediksi, data baru dapat ditambahkan dengan mulus. Hanya ada dua parameter yang diperlukan untuk mengimplementasikan KNN yaitu nilai K dan fungsi jarak (mis. Euclidean atau Manhattan dll.) Kontra Algoritma KNN tidak bekerja dengan baik dengan data dimensi tinggi karena dengan sejumlah besar dimensi, menjadi sulit bagi algoritma untuk menghitung jarak di setiap dimensi. Algoritma KNN memiliki cost prediksi tinggi untuk kumpulan data besar. Ini karena dalam dataset besar cost jarak penghitungan antara titik baru dan setiap titik yang ada menjadi lebih tinggi. Terakhir, algoritma KNN tidak bekerja dengan baik dengan fitur kategorikal karena sulit untuk menemukan jarak antara dimensi dengan fitur kategorikal.","title":"Pro dan Kontra KNN"},{"location":"knearest-neighbors/#implementasi-program","text":"Pada bagian ini, membahas bagaimana implementasi algoritma k-nearest neighbors dengan Scikit-Learn Python. Kita akan menggunakan dataset iris yang terkenal untuk contoh KNN ini. Dataset terdiri dari empat atribut: sepal-width, sepal-length, petal-width dan petal-length. Ini adalah atribut dari jenis spesifik tanaman iris. Fungsinya adalah untuk memprediksi kelas tanaman ini. Ada tiga kelas dalam dataset: Iris-setosa, Iris-versicolor dan Iris-virginica. Rincian lebih lanjut dari dataset tersedia di sini . Berikut langkah-langkah implementasi KNN: Download [1] Mengimpor Library python import pandas as pd import matplotlib.pyplot as plt import numpy as np [2] Mengimpor Dataset Untuk mengimpor dataset dan memuatnya dalam program, jalankan kode berikut: from sklearn import datasets from sklearn.linear_model import logistic_regression_path data_iris=datasets.load_iris() print(data_iris.data) print(data_iris.target) Hasilnya: [3] Split Dataset Untuk membagi dataset menjadi data training dan data testing dalam program, jalankan kode berikut: from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test=train_test_split(data_iris.data,data_iris.target,test_size=0.2) [4] Hitung Akurasi Untuk menghitung akurasi dalam program, jalankan kode berikut: from sklearn.neighbors import KNeighborsClassifier clf=KNeighborsClassifier(n_neighbors=3).fit(x_train,y_train) from sklearn.metrics import accuracy_score print(\"accuracy is = \", accuracy_score(y_test,clf.predict(x_test))) [5] Menampilkan grafik program Untuk grafik perhitungan dalam program, jalankan kode berikut: accuracy_values=[] for x in range(1,x_train.shape[0]): clf=KNeighborsClassifier(n_neighbors=x).fit(x_train,y_train) accuracy=accuracy_score(y_test,clf.predict(x_test)) accuracy_values.append([x,accuracy]) pass accuracy_values=np.array(accuracy_values) plt.plot(accuracy_values[:,0],accuracy_values[:,1]) plt.xlabel(\"K\") plt.ylabel(\"accuracy\") plt.show() Hasilnya : catatan: Grafik yang ditampilkan setiap menjalankan program akan dapat berbeda karena data diambil secara random.","title":"Implementasi Program"},{"location":"knearest-neighbors/#kesimpulan","text":"KNN adalah algoritma klasifikasi yang sederhana dan mudah diimplementasikan. Algoritma KNN telah banyak digunakan untuk menemukan kesamaan dokumen dan pengenalan pola. Untuk tutorial lainnya klik di sini .","title":"Kesimpulan"},{"location":"knearest-neighbors/#referensi","text":"[ 1 ] https://www.mkdocs.org [ 2 ] https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/","title":"Referensi"},{"location":"penyusun/","text":"Penyusun \u00b6 Nama : Abdur Rouf NIM : 170441100037 Mata Kuliah : Data Mining (B) Program Studi : Sistem Informasi Perguruan Tinggi : Universitas Trunojoyo Madura","title":"Penyusun"},{"location":"penyusun/#penyusun","text":"Nama : Abdur Rouf NIM : 170441100037 Mata Kuliah : Data Mining (B) Program Studi : Sistem Informasi Perguruan Tinggi : Universitas Trunojoyo Madura","title":"Penyusun"}]}